{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^a-z ]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()# lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(BAD_SYMBOLS_RE, '', text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([x for x in text.split() if x not in STOPWORDS])# delete stopwords from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-positive, 1-negative, 2-neutral\n",
    "df = pd.read_csv(\"inno/train.csv\")\n",
    "df[\"text\"] = df[\"text\"].apply(text_prepare)\n",
    "x = df[[\"text\", \"drug\"]]\n",
    "y = df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = df[\"text\"] + ' ' + df[\"drug\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2))\n",
    "features = tfidf.fit_transform(np.array(combined)) #fit transform to learn vocubulary and transform in matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = LogisticRegression(max_iter=1000)\n",
    "# regressor = MultinomialNB()\n",
    "# regressor = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "# fin_features = hstack([features, features1])\n",
    "regressor.fit(features, np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"inno/test.csv\")\n",
    "df1[\"text\"] = df1[\"text\"].apply(text_prepare)\n",
    "x1 = df1[\"text\"] + ' ' + df1[\"drug\"]\n",
    "y1 = df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_data = [[\"unique_hash\", \"sentiment\"]]\n",
    "for index, row in df1.iterrows():\n",
    "    print(index)\n",
    "    temp = tfidf.transform([row[\"text\"]])\n",
    "    temp1 = tfidf.transform([row[\"drug\"]])\n",
    "#     temp_fin = hstack([temp, temp1])\n",
    "    if regressor.predict(temp_fin)[0] != 2:\n",
    "        print(\"hello\",index)\n",
    "    fin_data.append([row[\"unique_hash\"], regressor.predict(temp_fin)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = tfidf.transform(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = regressor.predict(feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2914</th>\n",
       "      <th>2915</th>\n",
       "      <th>2916</th>\n",
       "      <th>2917</th>\n",
       "      <th>2918</th>\n",
       "      <th>2919</th>\n",
       "      <th>2920</th>\n",
       "      <th>2921</th>\n",
       "      <th>2922</th>\n",
       "      <th>2923</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
       "      <td>e747e6822c867571afe7b907b51f0f2ca67b0e1a</td>\n",
       "      <td>50b6d851bcff4f35afe354937949e9948975adf7</td>\n",
       "      <td>7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae</td>\n",
       "      <td>8b37d169dee5bdae27060949242fb54feb6a7f7f</td>\n",
       "      <td>b1950d27d94ceff4e9bf8c7d1fd4b11b35ede4d7</td>\n",
       "      <td>abafc5b6c5aac6f777cf265e5c7dd80fb793e6bc</td>\n",
       "      <td>e5550693e72a8335d723ca5fc64da91e1256fb0b</td>\n",
       "      <td>ee8c500f6402331ff12b0b29d943b6d1699a0b8d</td>\n",
       "      <td>d261600ba4fc022fac12748845deed56822ff195</td>\n",
       "      <td>...</td>\n",
       "      <td>04e2cede6d4f8fdf0f632c84bc991a9b5a9c6f94</td>\n",
       "      <td>b6c5b383282b31dca327caee258565a6726a32bb</td>\n",
       "      <td>6a6cb6fed029f72bae04b62d9481be3476114b18</td>\n",
       "      <td>937fca3e61512aaf82764139d01830e569cd45ba</td>\n",
       "      <td>32e9eca022d634fff808646b97fd18bfd64523de</td>\n",
       "      <td>ac6e60bec9162ae66effd29a0dc9ad11ff966df6</td>\n",
       "      <td>12afabb6210825308ead9894916abdfc912d7c43</td>\n",
       "      <td>021bb88c92a71229288304d691d53c3ff7004e4b</td>\n",
       "      <td>9936efcb83eded79fe9454df188edd7b96c6109e</td>\n",
       "      <td>05402df12d6769d7f38ab40e0b81464e65e1df0a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 2924 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0     \\\n",
       "0  9e9a8166b84114aca147bf409f6f956635034c08   \n",
       "1                                         2   \n",
       "\n",
       "                                       1     \\\n",
       "0  e747e6822c867571afe7b907b51f0f2ca67b0e1a   \n",
       "1                                         2   \n",
       "\n",
       "                                       2     \\\n",
       "0  50b6d851bcff4f35afe354937949e9948975adf7   \n",
       "1                                         2   \n",
       "\n",
       "                                       3     \\\n",
       "0  7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae   \n",
       "1                                         2   \n",
       "\n",
       "                                       4     \\\n",
       "0  8b37d169dee5bdae27060949242fb54feb6a7f7f   \n",
       "1                                         2   \n",
       "\n",
       "                                       5     \\\n",
       "0  b1950d27d94ceff4e9bf8c7d1fd4b11b35ede4d7   \n",
       "1                                         2   \n",
       "\n",
       "                                       6     \\\n",
       "0  abafc5b6c5aac6f777cf265e5c7dd80fb793e6bc   \n",
       "1                                         2   \n",
       "\n",
       "                                       7     \\\n",
       "0  e5550693e72a8335d723ca5fc64da91e1256fb0b   \n",
       "1                                         2   \n",
       "\n",
       "                                       8     \\\n",
       "0  ee8c500f6402331ff12b0b29d943b6d1699a0b8d   \n",
       "1                                         2   \n",
       "\n",
       "                                       9     ...  \\\n",
       "0  d261600ba4fc022fac12748845deed56822ff195  ...   \n",
       "1                                         2  ...   \n",
       "\n",
       "                                       2914  \\\n",
       "0  04e2cede6d4f8fdf0f632c84bc991a9b5a9c6f94   \n",
       "1                                         2   \n",
       "\n",
       "                                       2915  \\\n",
       "0  b6c5b383282b31dca327caee258565a6726a32bb   \n",
       "1                                         2   \n",
       "\n",
       "                                       2916  \\\n",
       "0  6a6cb6fed029f72bae04b62d9481be3476114b18   \n",
       "1                                         2   \n",
       "\n",
       "                                       2917  \\\n",
       "0  937fca3e61512aaf82764139d01830e569cd45ba   \n",
       "1                                         2   \n",
       "\n",
       "                                       2918  \\\n",
       "0  32e9eca022d634fff808646b97fd18bfd64523de   \n",
       "1                                         2   \n",
       "\n",
       "                                       2919  \\\n",
       "0  ac6e60bec9162ae66effd29a0dc9ad11ff966df6   \n",
       "1                                         2   \n",
       "\n",
       "                                       2920  \\\n",
       "0  12afabb6210825308ead9894916abdfc912d7c43   \n",
       "1                                         2   \n",
       "\n",
       "                                       2921  \\\n",
       "0  021bb88c92a71229288304d691d53c3ff7004e4b   \n",
       "1                                         2   \n",
       "\n",
       "                                       2922  \\\n",
       "0  9936efcb83eded79fe9454df188edd7b96c6109e   \n",
       "1                                         2   \n",
       "\n",
       "                                       2923  \n",
       "0  05402df12d6769d7f38ab40e0b81464e65e1df0a  \n",
       "1                                         2  \n",
       "\n",
       "[2 rows x 2924 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = [np.array(df1[\"unique_hash\"]), x3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = pd.DataFrame({'unique_hash':np.array(df1[\"unique_hash\"]), 'sentiment':x3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4.to_csv(\"inno/sub6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'autoimmune diseases tend come clusters gilenya feel good dont think wont change anything waste time energy im taking tysabri feel amazing symptoms dodgy color vision ive since always dont know dont k",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-cc5e06b11ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# X_test = sequence.pad_sequences(X_t, maxlen=max_words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'autoimmune diseases tend come clusters gilenya feel good dont think wont change anything waste time energy im taking tysabri feel amazing symptoms dodgy color vision ive since always dont know dont k"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 1000\n",
    "\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(combined, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_t, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Fit the model\n",
    "model.fit(X_train, y, epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 30)                60030     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 21        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 60,671\n",
      "Trainable params: 60,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4751 samples, validate on 528 samples\n",
      "Epoch 1/25\n",
      "4751/4751 [==============================] - 1s 170us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 2/25\n",
      "4751/4751 [==============================] - 0s 74us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 3/25\n",
      "4751/4751 [==============================] - 0s 85us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 4/25\n",
      "4751/4751 [==============================] - 0s 75us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 5/25\n",
      "4751/4751 [==============================] - 0s 74us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 6/25\n",
      "4751/4751 [==============================] - 0s 71us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 7/25\n",
      "4751/4751 [==============================] - 0s 78us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 8/25\n",
      "4751/4751 [==============================] - 0s 74us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 9/25\n",
      "4751/4751 [==============================] - 0s 75us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 10/25\n",
      "4751/4751 [==============================] - 0s 76us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 11/25\n",
      "4751/4751 [==============================] - 0s 72us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 12/25\n",
      "4751/4751 [==============================] - 0s 81us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 13/25\n",
      "4751/4751 [==============================] - 0s 85us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 14/25\n",
      "4751/4751 [==============================] - 0s 77us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 15/25\n",
      "4751/4751 [==============================] - 0s 81us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 16/25\n",
      "4751/4751 [==============================] - 0s 74us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 17/25\n",
      "4751/4751 [==============================] - 0s 76us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 18/25\n",
      "4751/4751 [==============================] - 0s 81us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 19/25\n",
      "4751/4751 [==============================] - 0s 77us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 20/25\n",
      "4751/4751 [==============================] - 0s 73us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 21/25\n",
      "4751/4751 [==============================] - 0s 77us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 22/25\n",
      "4751/4751 [==============================] - 0s 78us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 23/25\n",
      "4751/4751 [==============================] - 0s 77us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 24/25\n",
      "4751/4751 [==============================] - 0s 90us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n",
      "Epoch 25/25\n",
      "4751/4751 [==============================] - 0s 75us/step - loss: -9.6775 - acc: 0.1568 - val_loss: -9.7828 - val_acc: 0.1742\n"
     ]
    }
   ],
   "source": [
    "# def train_save_model(filename):\n",
    "#     df = pd.read_csv(filename)\n",
    "#     for i in range(len(df)):\n",
    "#         review1 = re.sub('[^a-zA-Z0-9#-]', ' ', df[\"String\"][i])\n",
    "#         review1 = (review1.lower()).split()\n",
    "#         review1 = [word1 for word1 in review1 if word1 not in set(stopwords.words('english'))]\n",
    "#         review1 = ' '.join(review1)\n",
    "#         df[\"String\"][i] = review1\n",
    "    \n",
    "#     train, test = train_test_split(df, test_size=0.2)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "from keras.models import load_model\n",
    "x_train = combined\n",
    "y_train = y\n",
    "\n",
    "#     print(x_train)\n",
    "\n",
    "\n",
    "vocab_size = 2000\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "#     save tokenizer model\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "x_train_mat = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "# x_test_mat = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "# encoder = LabelBinarizer()\n",
    "# encoder.fit(train[\"Class\"])\n",
    "\n",
    "# #     save encoder\n",
    "# np.save('encoder_classification_2018-10-04_16:52.npy'.format(now.strftime(\"%Y-%m-%d_%H:%M\")), encoder.classes_)\n",
    "\n",
    "# y_train_enco = encoder.transform(y_train)\n",
    "# y_test_enco = encoder.transform(y_test)\n",
    "\n",
    "# print(encoder.classes_)\n",
    "# num_labels = len(encoder.classes_)\n",
    "# print(num_labels)\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# print(y_train_enco)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train_mat, y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=25,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ou1 = model.predict(tokenizer.texts_to_matrix(x1, mode='tfidf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
